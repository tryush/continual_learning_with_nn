{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ab845b5-5aba-4090-a6fc-84eab540e902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-05 17:40:25.200597: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-05 17:40:25.304376: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-05 17:40:25.309585: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-02-05 17:40:25.309607: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-02-05 17:40:25.332636: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-05 17:40:25.934610: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-02-05 17:40:25.934668: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-02-05 17:40:25.934675: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43ffe4b-366c-460c-8262-768977b1970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_g1 = pd.read_excel(\"/home/ubuntu/training/ayush/continual_learning/G1.xlsx\")\n",
    "data_g2 = pd.read_excel(\"/home/ubuntu/training/ayush/continual_learning/G2.xlsx\")\n",
    "data_g3 = pd.read_excel(\"/home/ubuntu/training/ayush/continual_learning/G3.xlsx\")\n",
    "\n",
    "data_g1.shape, data_g2.shape, data_g3.shape\n",
    "\n",
    "data_g1.drop('Unnamed: 0', axis =1, inplace = True)\n",
    "data_g2.drop('Unnamed: 0', axis =1, inplace = True)\n",
    "data_g3.drop('Unnamed: 0', axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68991b4-1dfe-4121-900b-dabe2c071cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_g2['text'] = data_g2['text'].astype(str)\n",
    "\n",
    "data_g1['tags'] = data_g1['tags'].apply(lambda x: re.sub(r\"[^a-zA-Z_]\", \" \", x))\n",
    "data_g2['tags'] = data_g2['tags'].apply(lambda x: re.sub(r\"[^a-zA-Z_]\", \" \", x))\n",
    "data_g3['tags'] = data_g3['tags'].apply(lambda x: re.sub(r\"[^a-zA-Z_]\", \" \", x))\n",
    "\n",
    "data_g1['tags'] = data_g1['tags'].apply(lambda x: ', '.join(word.strip() for word in set(x.split())))\n",
    "data_g2['tags'] = data_g2['tags'].apply(lambda x: ', '.join(word.strip() for word in set(x.split())))\n",
    "data_g3['tags'] = data_g3['tags'].apply(lambda x: ', '.join(word.strip() for word in set(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fafa26ff-d035-4661-8d5b-01ed1f4fa29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5884, 3), (1472, 3), (5164, 3), (1291, 3), (5021, 3), (1256, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_g1_train, data_g1_test = train_test_split(data_g1, test_size = 0.2, random_state = 42)\n",
    "data_g2_train, data_g2_test = train_test_split(data_g2, test_size = 0.2, random_state = 42)\n",
    "data_g3_train, data_g3_test = train_test_split(data_g3, test_size = 0.2, random_state = 42)\n",
    "data_g1_train.shape, data_g1_test.shape, data_g2_train.shape, data_g2_test.shape, data_g3_train.shape, data_g3_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbe67e27-280d-4a2f-afea-50716f805b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    max_words = 10000 \n",
    "    max_length = 300  \n",
    " \n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(data['text'])\n",
    " \n",
    "    sequences = tokenizer.texts_to_sequences(data['text'])\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    " \n",
    "    mlb = MultiLabelBinarizer()\n",
    "    encoded_tags = mlb.fit_transform(data['tags'].apply(lambda x: set(x.split(', '))))\n",
    " \n",
    "    return {'text': padded_sequences, 'tags': encoded_tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87e85d61-fc61-4b97-ace6-0fee485d7dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_model(data, model_name, vocab_size, embedding_dim, max_length, num_classes):\n",
    "    model = keras.Sequential([\n",
    "        layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(data['text'], data['tags'], epochs=50)\n",
    "\n",
    "    model.save(f'/home/ubuntu/training/ayush/continual_learning/models/{model_name}.h5')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bc53bc0-0418-4608-baac-c10d37f7c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_train = preprocess_data(data_g1_train)\n",
    "data1_test = preprocess_data(data_g1_test)\n",
    "\n",
    "data2_train = preprocess_data(data_g2_train)\n",
    "data2_test = preprocess_data(data_g2_test)\n",
    "\n",
    "data3_train = preprocess_data(data_g3_train)\n",
    "data3_test = preprocess_data(data_g3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27dc05b8-7f48-476d-9e6f-0d080c9bc295",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = {\n",
    "    'text': np.concatenate([data1_train['text'], data2_train['text'], data3_train['text']]),\n",
    "    'tags': np.concatenate([data1_train['tags'], data2_train['tags'], data3_train['tags']])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cde68d-12e6-4327-8c71-cf6f2a63c447",
   "metadata": {},
   "source": [
    "## Training model on combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b499a89-fb4d-460a-afa5-acc24292c6b6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-05 17:41:14.831105: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-02-05 17:41:14.831129: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-02-05 17:41:14.831146: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-0-71): /proc/driver/nvidia/version does not exist\n",
      "2024-02-05 17:41:14.831424: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503/503 [==============================] - 2s 3ms/step - loss: 0.3978 - accuracy: 0.6051\n",
      "Epoch 2/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.2420 - accuracy: 0.7858\n",
      "Epoch 3/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.1502 - accuracy: 0.8643\n",
      "Epoch 4/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0864 - accuracy: 0.8983\n",
      "Epoch 5/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0470 - accuracy: 0.9147\n",
      "Epoch 6/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0255 - accuracy: 0.9207\n",
      "Epoch 7/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0149 - accuracy: 0.9228\n",
      "Epoch 8/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0102 - accuracy: 0.9253\n",
      "Epoch 9/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0076 - accuracy: 0.9269\n",
      "Epoch 10/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0064 - accuracy: 0.9245\n",
      "Epoch 11/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0052 - accuracy: 0.9250\n",
      "Epoch 12/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9290\n",
      "Epoch 13/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9266\n",
      "Epoch 14/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9272\n",
      "Epoch 15/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0036 - accuracy: 0.9258\n",
      "Epoch 16/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0032 - accuracy: 0.9271\n",
      "Epoch 17/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9276\n",
      "Epoch 18/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0036 - accuracy: 0.9264\n",
      "Epoch 19/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0036 - accuracy: 0.9243\n",
      "Epoch 20/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9223\n",
      "Epoch 21/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9213\n",
      "Epoch 22/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9246\n",
      "Epoch 23/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9207\n",
      "Epoch 24/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9216\n",
      "Epoch 25/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9222\n",
      "Epoch 26/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0032 - accuracy: 0.9184\n",
      "Epoch 27/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0024 - accuracy: 0.9174\n",
      "Epoch 28/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9203\n",
      "Epoch 29/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9179\n",
      "Epoch 30/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0027 - accuracy: 0.9198\n",
      "Epoch 31/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9208\n",
      "Epoch 32/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0022 - accuracy: 0.9244\n",
      "Epoch 33/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0027 - accuracy: 0.9196\n",
      "Epoch 34/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0020 - accuracy: 0.9192\n",
      "Epoch 35/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0022 - accuracy: 0.9179\n",
      "Epoch 36/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0018 - accuracy: 0.9177\n",
      "Epoch 37/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0020 - accuracy: 0.9196\n",
      "Epoch 38/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0022 - accuracy: 0.9137\n",
      "Epoch 39/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0024 - accuracy: 0.9104\n",
      "Epoch 40/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0021 - accuracy: 0.9081\n",
      "Epoch 41/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0016 - accuracy: 0.9126\n",
      "Epoch 42/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0018 - accuracy: 0.9116\n",
      "Epoch 43/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0016 - accuracy: 0.9144\n",
      "Epoch 44/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0016 - accuracy: 0.9067\n",
      "Epoch 45/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0016 - accuracy: 0.9094\n",
      "Epoch 46/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0016 - accuracy: 0.9096\n",
      "Epoch 47/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9124\n",
      "Epoch 48/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0023 - accuracy: 0.9151\n",
      "Epoch 49/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0015 - accuracy: 0.9148\n",
      "Epoch 50/50\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0018 - accuracy: 0.9144\n"
     ]
    }
   ],
   "source": [
    "model_comb = create_and_train_model(data_combined, 'model_combined', vocab_size = 10000, embedding_dim = 50, max_length = 300, num_classes = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aec321-e259-4129-aba2-43737323070e",
   "metadata": {},
   "source": [
    "## performance on combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45f2031a-2591-496e-b965-1ba4107b1a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_combined = {\n",
    "    'text': np.concatenate([data1_test['text'], data2_test['text'], data3_test['text']]),\n",
    "    'tags': np.concatenate([data1_test['tags'], data2_test['tags'], data3_test['tags']])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71fb4f96-dae1-4520-a7c2-78d117423de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comb = keras.models.load_model(\"/home/ubuntu/training/ayush/continual_learning/models/model_combined.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c31ed5ba-9b89-478c-83f7-9bc534d1204c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 0s 1ms/step\n",
      "F1 score for tag 'allergy_name': 0.05952380952380952\n",
      "F1 score for tag 'cancer': 0.2279245283018868\n",
      "F1 score for tag 'chronic_disease': 0.4484278577725536\n",
      "F1 score for tag 'treatment': 0.5328269126424309\n",
      "\n",
      "Overall Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "   allergy_name       0.10      0.04      0.06       116\n",
      "         cancer       0.24      0.22      0.23       684\n",
      "chronic_disease       0.50      0.41      0.45      1883\n",
      "      treatment       0.55      0.52      0.53      1885\n",
      "\n",
      "      micro avg       0.47      0.42      0.44      4568\n",
      "      macro avg       0.34      0.30      0.32      4568\n",
      "   weighted avg       0.47      0.42      0.44      4568\n",
      "    samples avg       0.47      0.42      0.44      4568\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "tags_order = ['allergy_name', 'cancer', 'chronic_disease', 'treatment']\n",
    "\n",
    "def evaluate_model(model, test_data, tags_order):\n",
    "    predictions = model.predict(test_data['text'])\n",
    "\n",
    "    binary_predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "    true_labels = test_data['tags']\n",
    "\n",
    "    f1_scores = []\n",
    "    for i in range(len(tags_order)):\n",
    "        tag_f1 = f1_score(true_labels[:, i], binary_predictions[:, i])\n",
    "        print(f\"F1 score for tag '{tags_order[i]}': {tag_f1}\")\n",
    "        f1_scores.append(tag_f1)\n",
    "        \n",
    "    print(\"\\nOverall Classification Report:\\n\", classification_report(true_labels, binary_predictions, target_names=tags_order))\n",
    "\n",
    "    return f1_scores\n",
    "\n",
    "f1_scores_data1 = evaluate_model(model_comb, data_test_combined, tags_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f1a2f-34f7-47e6-aff2-020d2d089f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
